# 存储架构设计

## 概览

- RocksWrapper 单例：NeoKV 如何封装和初始化 RocksDB
- 8 个 Column Family 的设计：每个 CF 存什么数据、为什么这样配置
- Key 编码体系：mem-comparable 编码的原理与实现
- SplitCompactionFilter：Region Split 后的数据清理

## 一个 RocksDB 实例，八个 Column Family

上一章我们了解了 RocksDB 的核心特性。现在来看 NeoKV 如何使用它。

NeoKV 的每个 Store 节点只有一个 RocksDB 实例，通过 `RocksWrapper` 单例（`include/engine/rocks_wrapper.h`）管理。这个决策在第三章讨论 Multi-Raft 存储时已经详细解释过——所有 Region 共享一个 RocksDB，利用其 group commit 和统一的 compaction 来消化 Multi-Raft 带来的 I/O 压力。

`RocksWrapper::init()` 打开或创建数据库时，使用的是 `rocksdb::TransactionDB::Open()` 而非普通的 `DB::Open()`，这为 NeoKV 提供了行级锁和事务支持。事务锁超时配置为 20 秒，避免死锁时无限等待。

在 DB 级别的配置上，NeoKV 默认启用 24 个后台线程（其中 20 个用于 Compaction，2 个用于 Flush），单次 Compaction 最多并行 4 个子任务。WAL 保留 10 分钟（`WAL_ttl_seconds = 600`），最大打开文件数 1024。Block Cache 默认 8GB，使用 LRU 策略，分 8 个 shard 以减少锁竞争。这些参数都可以通过 gflags 运行时调整。

这个 RocksDB 实例内部划分为 8 个 Column Family，每个 CF 针对自己承载的数据特点做了专门调优。接下来逐一介绍。

**RAFT_LOG_CF** 存储所有 Region 的 Raft 日志条目、日志元信息和 Raft 元数据（term、votedfor）。它使用 9 字节的 Prefix Bloom Filter（`region_id` 8 字节 + 类型字节 1 字节），让同一 Region 的同类数据共享一个 Bloom 条目。Compaction 优先级设为 `kOldestLargestSeqFirst`——优先合并最旧的数据，这与 Raft 日志"追加写入、旧数据先被截断"的访问模式完美匹配。Write Buffer 配置为 128MB × 6，较大的 buffer 减少了 flush 频率。已截断的旧日志通过我们在第三章讨论过的 `RaftLogCompactionFilter` 惰性清理（该 Filter 在自定义 Raft 日志存储层注册，不在 CF 选项中直接配置）。

**BIN_LOG_CF** 是 Binlog（变更数据捕获）的存储，使用 8 字节的时间戳前缀 Bloom Filter。它是唯一使用 FIFO Compaction 策略的 CF——只保留最近的数据，总大小超过 100GB 时自动删除最旧的 SST 文件。这非常适合 Binlog 的特性：我们只关心最近的变更历史，旧数据可以安全丢弃。压缩使用 LZ4。

**DATA_CF** 是最重要也是配置最复杂的 CF。它存储所有 Redis 复合类型的子键数据——Hash 的 field、Set 的 member、List 的 element、ZSet 的 member——以及旧编码方式（Phase 1）的 String 数据。Prefix Bloom Filter 为 16 字节（`region_id` 8 字节 + `index_id` 8 字节），MemTable 也开启了 Prefix Bloom（占 MemTable 大小的 10%），加速内存中的前缀查询。Compaction 优先级为 `kMinOverlappingRatio`——选择与下层重叠最少的文件合并，减少写放大。压缩分层：L0 不压缩，L1-L6 使用 LZ4，最底层可选启用 ZSTD + 字典压缩。

DATA_CF 有两个特殊配置值得单独说。**SST Partitioner**（可选启用）通过 `FixedPrefixFactory(8)` 保证每个 SST 文件只包含一个 Region 的数据——8 字节的 `region_id` 作为分区边界。这让 Region Split 后的数据清理更高效：整个 SST 文件如果完全属于被清理的范围，可以直接丢弃而不需要逐条过滤。**Auto Compaction 初始禁用**——Store 启动时先关闭自动 compaction，等到与 MetaServer 的心跳成功后才开启。这避免了启动阶段 compaction 与 Region 初始化竞争 I/O 和 CPU 资源。

DATA_CF 还注册了 `SplitCompactionFilter`，用于在 Region Split 后清理超出新范围的数据。我们稍后详细讨论这个 Filter 的实现。

**METAINFO_CF** 存储 Region 的边界信息、Peer 列表等元数据。这些数据量很小但访问频繁，所以使用 1 字节的分类前缀 Bloom Filter（不同的前缀字节代表不同类别的元数据），Block Size 设为 4096 字节（比其他 CF 的 64KB 小得多，因为元数据条目本身就很小），Compaction 优先级为 `kOldestSmallestSeqFirst`。

**REDIS_METADATA_CF** 存储每个 Redis key 的元信息——类型、过期时间、版本号、元素数量。String 类型的值也内联存储在这里（详见下一章）。配置与 DATA_CF 类似：16 字节 Prefix Bloom Filter，MemTable Prefix Bloom 10%，SST Partitioner 按 `region_id` 分区，L0 不压缩、L1-L6 LZ4。但它**没有** Compaction Filter——过期的 Redis key 通过被动过期（读时检查）和主动清理（`RedisTTLCleaner` 后台线程）处理，而不是在 compaction 时清理。

**REDIS_ZSET_SCORE_CF** 存储 Sorted Set 的按分数排序的二级索引。配置与 REDIS_METADATA_CF 基本相同。之所以独立成一个 CF，是因为 ZSet 需要按 member 和按 score 两种顺序访问数据——member 索引放在 DATA_CF，score 索引放在这里，各自维护独立的 MemTable 和 SST 文件，迭代器互不干扰。

**COLD_DATA_CF 和 COLD_BINLOG_CF** 是两个可选的冷存储 CF，存储在独立的 RocksDB 实例（`_cold_txn_db`）中，只在配置了冷存储路径时才启用。冷数据通过 SST Ingestion 导入。

## Key 编码：让字节序为你工作

RocksDB 内部按字节序（lexicographic order）排列 key。这意味着如果你想让业务上的排序需求（比如"同一 Region 的数据排在一起"、"同一 key 的子元素排在一起"）直接由 RocksDB 的底层排序保证，你需要设计一种编码方式，让编码后的字节序与业务排序一致。NeoKV 为此设计了一套 **mem-comparable 编码**。

核心工具是 `KeyEncoder`（`include/common/key_encoder.h`）。对于无符号整数，处理很简单——转为大端序（big-endian）就够了，因为大端序的字节比较等价于数值比较。但有符号整数就没这么直观了。

问题出在二进制补码表示上。在补码中，负数的最高位是 1，正数是 0。如果直接按字节比较，`-1`（0xFFFFFFFFFFFFFFFF）会大于 `1`（0x0000000000000001），这显然不对。`KeyEncoder::encode_i64()` 的解决方案是翻转符号位：

```cpp
static uint64_t encode_i64(int64_t val) {
    uint64_t uval = static_cast<uint64_t>(val);
    return uval ^ (1ULL << 63);
}
```

翻转后，`-1` 变成 `0x7FFFFFFFFFFFFFFF`，`1` 变成 `0x8000000000000001`，`0x7F... < 0x80...`，排序正确。再转为大端序追加到 key 中，就得到了按字节比较等价于数值比较的编码。

`MutTableKey`（`include/common/mut_table_key.h`）是构建编码 key 的 fluent API。它内部维护一个 `std::string`，提供 `append_i64`、`append_u64`、`append_u32`、`append_u16`、`append_u8`、`append_string` 等方法，每次调用都将编码后的字节追加到内部 buffer 中。`TableKey`（`include/common/table_key.h`）则是只读的 key 解码器，包装 `rocksdb::Slice`，提供 `extract_i64`、`extract_u64`、`extract_u16` 等反向解码方法。

NeoKV 中所有 Redis 数据的 key 都遵循统一的前缀格式：

```
┌──────────────────────────────────────────────────────────────┐
│                        RocksDB Key                           │
├──────────┬──────────┬──────┬────────────┬───────┬───────────┤
│region_id │ index_id │ slot │user_key_len│user_key│  suffix   │
│  8 字节  │  8 字节  │2 字节│   4 字节   │ 变长  │  变长     │
├──────────┴──────────┤      │            │       │           │
│   16 字节前缀        │      │            │       │           │
│  (Bloom Filter 粒度) │      │            │       │           │
└─────────────────────┴──────┴────────────┴───────┴───────────┘
```

前 16 个字节（`region_id` + `index_id`）正好对应 DATA_CF 和 REDIS_METADATA_CF 的 Prefix Bloom Filter 长度。这不是巧合——Bloom Filter 的前缀长度是根据 key 的编码格式精心选择的。`region_id` 用 `append_i64` 编码（符号位翻转 + 大端），保证同一 Region 的所有 key 在 RocksDB 中物理相邻。`index_id` 同样如此。`slot` 用 2 字节大端无符号整数。`user_key_len` + `user_key` 是 Redis 用户 key 的变长编码。suffix 部分因 CF 和数据类型而异——我们在下一章详细讨论。

为什么 Prefix Bloom Filter 选 16 字节而不是 8 字节（只用 `region_id`）？虽然 NeoKV 当前只有一张内部表（`__redis__.kv`），但 16 字节前缀为未来可能支持多表（多个 `index_id`）留了扩展空间，且不影响当前性能——目前每个 Region 只有一个 `index_id`，所以 16 字节前缀在过滤粒度上等同于 8 字节。

## SplitCompactionFilter：Split 后的清理工

Region Split 后，原 Region 的 key 范围缩小了，但 RocksDB 中仍然存在超出新范围的数据——它们现在属于新 Region，但在旧 Region 的 SST 文件中还有一份副本。`SplitCompactionFilter`（`include/engine/split_compaction_filter.h`）在 compaction 时清理这些数据。

它的实现很直接。内部维护了一个 `std::unordered_map<int64_t, std::string>`（`region_id` → `end_key` 的映射），受 `std::mutex` 保护。Region Split 完成后，Store 调用 `set_filter_region_info()` 注册新的 end_key。当 compaction 过程中遍历到一个 KV 对时，Filter 从 key 的前 8 字节解码出 `region_id`，查找该 Region 的 `end_key`，然后比较 key 的 suffix 部分（16 字节之后）是否超出范围——如果 `suffix >= end_key`，说明这条数据已经不属于该 Region 了，删除。

如果一个 Region 是尾部 Region（`end_key` 为空，表示无上界），则不注册到 Filter 中——尾部 Region 不需要过滤。

虽然 compaction 线程会频繁调用 `Filter()`，但锁的粒度很小（只是一次 `unordered_map` 查找），实际开销可控。而且 Filter 的触发频率完全取决于 compaction 的节奏——不是每次 Region Split 都立即清理，而是在后续的 compaction 中顺便完成。这和第三章讨论的 Raft 日志惰性删除是同一个思路：不急着删除，让 compaction 在合适的时候顺手处理。

## 检验你的理解

- NeoKV 所有 Region 共享一个 RocksDB 实例。如果某个 Region 的数据量远大于其他 Region，会对 compaction 产生什么影响？SST Partitioner 如何缓解这个问题？
- 为什么 REDIS_METADATA_CF 没有 Compaction Filter？如果给它加一个 Filter 来清理过期 key，会有什么好处和风险？
- DATA_CF 的 Auto Compaction 在启动时禁用，心跳成功后才启用。如果 MetaServer 长时间不可用，Store 的 DATA_CF 会发生什么？
- `region_id` 使用 `encode_i64`（符号位翻转 + 大端）而不是简单的大端 `uint64`。在什么情况下这个区别会导致不同的排序结果？
- `SplitCompactionFilter` 在 compaction 时加锁查询 `_region_end_keys`。如果 Region 数量很多（比如 10000 个），这个锁会不会成为瓶颈？有什么优化方案？

---

> 下一章：[07-Redis 存储编码](./07-Redis存储编码.md) — 我们将深入 Redis 数据类型在 RocksDB 中的编码方式，包括 Metadata CF、Subkey CF 和 Version-based Lazy Deletion。
